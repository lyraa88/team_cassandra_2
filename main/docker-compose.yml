services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: kc-hdfs-namenode
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    environment:
      - CLUSTER_NAME=kc-hadoop
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - FORMAT=true
    ports:
      - "9870:9870"   # NameNode UI
    volumes:
      - ../data/hdfs/namenode:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: kc-hdfs-datanode
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=datanode:9864
      - HDFS_CONF_dfs_datanode_ipc_address=datanode:9867
    ports:
      - "9865:9864"   # DataNode UI
    volumes:
      - ../data/hdfs/datanode:/hadoop/dfs/data
    depends_on:
      - namenode

  spark-master:
    image: bitnami/spark:3.5
    container_name: kc-spark-master
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
    ports:
      - "8081:8080"   # Spark Master UI
      - "7077:7077"   # Spark RPC
    depends_on:
      - namenode
      - datanode
    volumes:
      - ./spark_app:/opt/bitnami/spark/app  # 스파크 앱 지정

  spark-worker:
    image: bitnami/spark:3.5
    container_name: kc-spark-worker
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://kc-spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master

  kafka:
    image: bitnami/kafka:3.7
    container_name: kc-kafka
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    ports:
      - "29092:29092"   # external
      - "9092:9092"     # internal (optional expose)
    environment:
      - KAFKA_CFG_NODE_ID=${KAFKA_BROKER_ID}
      - KAFKA_KRAFT_CLUSTER_ID=${KAFKA_CLUSTER_ID}
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_INTERNAL_LISTENER},EXTERNAL://${KAFKA_EXTERNAL_LISTENER}
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=${KAFKA_BROKER_ID}@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
    volumes:
      - ../data/kafka:/bitnami/kafka

  kafka-init:
    image: bitnami/kafka:3.7
    depends_on:
      - kafka
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Waiting for Kafka..."
        sleep 30
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic logs_raw --partitions 2 --replication-factor 1
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic logs_clean --partitions 1 --replication-factor 1
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic logs_dlq --partitions 1 --replication-factor 1
    networks:
      - cassandra_etl_net

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kc-kafka-ui
    restart: unless-stopped
    networks: [ cassandra_etl_net ]
    ports:
      - "8085:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=
    depends_on:
      - kafka

  cassandra:
    image: cassandra:latest
    container_name: cassandra
    ports:
      - "9042:9042"
    volumes:
      - ../data/cassandra:/var/lib/cassandra
    networks:
      - cassandra_etl_net

  spark-app:
    image: bitnami/spark:3.5
    container_name: kc-spark-app
    restart: on-failure
    networks: [ cassandra_etl_net ]
    depends_on:
      - kafka
      - spark-master
    volumes:
      - ./spark_app:/opt/bitnami/spark/app
      - ./data/spark_events:/tmp/spark-events # 이 줄을 추가

    command: >
      /opt/bitnami/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      --master spark://kc-spark-master:7077
      --conf spark.eventLog.enabled=true
      /opt/bitnami/spark/app/stream_processor.py

networks:
  cassandra_etl_net:
    external: true